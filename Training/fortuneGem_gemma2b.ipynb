{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning Gemma"
      ],
      "metadata": {
        "id": "rS_YFwruPNrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up"
      ],
      "metadata": {
        "id": "Ez3qunY4PRzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U transformers\n",
        "%pip install -U datasets\n",
        "%pip install -U accelerate\n",
        "%pip install -U peft\n",
        "%pip install -U trl\n",
        "%pip install -U bitsandbytes\n",
        "%pip install -U wandb"
      ],
      "metadata": {
        "id": "vtlKHZBPPYZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import json\n",
        "import time\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, AutoPeftModelForCausalLM, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "RUrxTpq9PaMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "MuxlueDxPgK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = \"google/gemma-2b\"\n",
        "new_model = \"fortuneGem_gemma2b\"\n",
        "dataset_name = \"junonnong/daily_horoscope_kr\""
      ],
      "metadata": {
        "id": "QPaJdA2zPvQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the model and tokenizer"
      ],
      "metadata": {
        "id": "Vl9Wzck4QGhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "id": "CYri1BWdQW6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "___QBY81QH3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting the linear modules"
      ],
      "metadata": {
        "id": "21RSbJ2RQpdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes as bnb\n",
        "\n",
        "def find_all_linear_names(model):\n",
        "    cls = bnb.nn.Linear4bit\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)\n",
        "\n",
        "modules = find_all_linear_names(model)\n",
        "modules"
      ],
      "metadata": {
        "id": "ujgSC1aoQyjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset"
      ],
      "metadata": {
        "id": "IeBRw6HJQ383"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNoj2JpsWEk5"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 데이터셋 로드\n",
        "dataset = load_dataset(dataset_name, data_files='datasets.csv', split=\"all\")\n",
        "\n",
        "# 데이터셋의 구조 확인\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "2s088mR_Q7jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_instruction(example):\n",
        "\n",
        "    text = f\"\"\"user\\n{example[\"instruction\"]}\\n{example[\"birthday\"]} 천간지지: {example[\"birthday_saju\"]}\\n{example[\"Date\"]} 천간지지: {example[\"fortune_saju\"]}\\nmodel\\n{example[\"Output\"]}\"\"\"\n",
        "\n",
        "    return {'prompt': text}\n",
        "\n",
        "dataset = dataset.map(format_instruction)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "GTWLqCwyRB_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
        "dataset = dataset.train_test_split(test_size=0.2)\n",
        "train_data = dataset[\"train\"]\n",
        "test_data = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "TbM-0RibRGi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "K4tIGg9zRxV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=modules,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    dataset_text_field=\"prompt\",\n",
        "    peft_config=lora_config,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=100,\n",
        "        learning_rate=2e-5,\n",
        "        fp16=True,\n",
        "        logging_steps=100,\n",
        "        output_dir=\"outputs\",\n",
        "    )\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "eOYxl5eHRK-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(query: str, model, tokenizer):\n",
        "\n",
        "  prompt_template = \"\"\"user\n",
        "  {query}\n",
        "\n",
        "  model\n",
        "  \"\"\"\n",
        "  prompt = prompt_template.format(query=query)\n",
        "  encodeds = tokenizer(prompt, return_tensors=\"pt\")\n",
        "  model_inputs = encodeds.to(\"cuda:0\")\n",
        "  generated_ids = model.generate(**encodeds,max_new_tokens=500)\n",
        "  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "  return decoded"
      ],
      "metadata": {
        "id": "Th7VmPN-R9kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"\"\"1997년10월29일생의 천간지지와 2024년9월27일의 천간지지를 토대로 해당 날짜의 운세를 10줄 이하로만 알려줘. 다른 설명은 필요 없어.\n",
        "1997-10-29 천간지지: 정축년 경술월 갑진일\n",
        "2024-09-27 천간지지: 갑진년 계유월 갑오일\n",
        "\"\"\"\n",
        "\n",
        "result = get_completion(query=query,\n",
        "                        model=trainer.model,\n",
        "                        tokenizer=tokenizer)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "upaEZdToR5NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merging the Base model with Adapter"
      ],
      "metadata": {
        "id": "LN3EcTR4Sa7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.save_pretrained(\"new_adapter_model\")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model,\n",
        "                                                  low_cpu_mem_usage=True,\n",
        "                                                  return_dict=True,\n",
        "                                                  torch_dtype=torch.float16,\n",
        "                                                  device_map={\"\":0}\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, \"new_adapter_model\")\n",
        "\n",
        "model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "XByHzF8wSd7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the model"
      ],
      "metadata": {
        "id": "zTU74ip_SN2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(new_model)\n",
        "model.push_to_hub(new_model, use_temp_dir=False)\n",
        "\n",
        "tokenizer.save_pretrained(new_model)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
      ],
      "metadata": {
        "id": "Ucg07bfqR_3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert model to MediaPipe format for on-device deployment"
      ],
      "metadata": {
        "id": "3auWjmiCTG49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "KlDxVR2PTICc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mediapipe.tasks.python.genai import converter\n",
        "\n",
        "config = converter.ConversionConfig(\n",
        "  input_ckpt='/content/fortuneGem_gemma2b',\n",
        "  ckpt_format=\"safetensors\",\n",
        "  model_type=\"GEMMA_2B\",\n",
        "  backend='gpu',\n",
        "  output_dir='/content/fortuneGem_gemma2b/intermediate/fine_tuned_fortune_gemma2b',\n",
        "  combine_file_only=False,\n",
        "  vocab_model_file=\"/content/fortuneGem_gemma2b\",\n",
        "  output_tflite_file=f'/content/fortuneGem_gemma2b/fine_tuned_fortune_gemma2b/fortunegem.bin',\n",
        ")\n",
        "\n",
        "converter.convert_checkpoint(config)\n",
        "\n",
        "print(\"Model converted successfully.\")"
      ],
      "metadata": {
        "id": "sTWO1WIoTK94"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}